{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a72d95",
   "metadata": {},
   "source": [
    "\n",
    "# House Prices - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63285527",
   "metadata": {},
   "source": [
    "In this notebook we will create a model to predict house prices for the **Kaggle** competition [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).  \n",
    "\n",
    "The dataset used for this, is the Ames Housing dataset, but modified for learning purposes.  \n",
    "And the evaluation metric used will be the Root Mean Squared Error (RMSE), on the log transformed price.  \n",
    "\n",
    "The competition provides the following:  \n",
    "- **train.csv**\n",
    "> Training dataset comprised of the house's features and their sale price\n",
    "- **test.csv**\n",
    "> Test dataset with only the house's features but not their sale price.  \n",
    "> Will act as unseen data and the final submitted prediction will be done on it.\n",
    "- **data_description.txt**\n",
    "> Text file describing each feature and their possible values (when categorical).\n",
    "\n",
    "\n",
    "Our real objective with this project is not to create the best performing model possible, but to create different types of models and approaches, and compare their performances.  \n",
    "For this, we will first do EDA on the data as well as some preprocessing and feature engineering, and then build models using the prepared data, compare them, and make a final prediction with the chosen one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484616",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c4a65",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884b870",
   "metadata": {},
   "source": [
    "Before we start we need to import the resources (libraries, modules, etc.) that we will be using.  \n",
    "For clarity, all imports used in this notebook will be done on this cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0efa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import joblib\n",
    "\n",
    "# TensorFlow Decision Forests (TFDF)\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "# CatBoost \n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Normalization, StringLookup, CategoryEncoding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366554c",
   "metadata": {},
   "source": [
    "### 1.2 Global settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1264493",
   "metadata": {},
   "source": [
    "We will set a fixed random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility (for Python, NumPy, and TensorFlow)\n",
    "tf.keras.utils.set_random_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a51658",
   "metadata": {},
   "source": [
    "As the data provided has many features, we will set up **Pandas** to display all columns and rows, making it easier to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d318d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155ca6c",
   "metadata": {},
   "source": [
    "### 1.3 Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18a772",
   "metadata": {},
   "source": [
    "We will load the datasets provided, **train.csv** and **test.csv**, as **Pandas** dataframes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a491039",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainrawdata_path = '../data/raw/train.csv' # Relative path to the training dataset\n",
    "traindf = pd.read_csv(trainrawdata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdccb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "testrawdata_path = '../data/raw/test.csv' # Relative path to the test dataset\n",
    "testdf = pd.read_csv(testrawdata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d47b3",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5a08d",
   "metadata": {},
   "source": [
    "### 2.1 Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b902480",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13b49b",
   "metadata": {},
   "source": [
    "We can see that our training dataset has 1460 entries, with 79 features (81 columns but one is **Id** and another is the target variable **SalePrice**).  \n",
    "Similarly, the test dataset provided has 1459 entries, with 79 features (80 columns couting **Id**), and as mentioned earlier, without target variable.  \n",
    "\n",
    "Features are stored as 3 different data types: **int64**, **float64** and **object** (string).\n",
    "\n",
    "In both datasets there seems to be missing data in some features, which we will explore and deal with later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c6db9",
   "metadata": {},
   "source": [
    "Let's take a quick look at a few examples of our data to see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31d80f",
   "metadata": {},
   "source": [
    "After this first look we see that we have a mixed set of data, some columns are quantitative (numerical) like our target variable **SalePrice** or **LotArea** while many others are qualitative (categorical).  \n",
    "Some of the categorical features are nominal but there are also ordinal variables like **OverallCond** which rates from 1 to 10 the overall condition of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaec3c",
   "metadata": {},
   "source": [
    "### 2.2 Target variable analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05bbb6",
   "metadata": {},
   "source": [
    "Our target variable is **SalePrice**, a numerical variable which shows the price at what each house got sold, in US Dollars.  \n",
    "We should take a first look at its distribution:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e26f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55abf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of SalePrice\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(traindf['SalePrice'], kde=True)\n",
    "plt.title('Distribution of SalePrice')\n",
    "plt.xlabel('SalePrice')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# Calculate skewness and kurtosis\n",
    "print(f'Skewness of SalePrice: {traindf[\"SalePrice\"].skew()}')\n",
    "print(f'Kurtosis of SalePrice: {traindf[\"SalePrice\"].kurt()}')\n",
    "# Boxplot of SalePrice\n",
    "plt.figure(figsize=(8, 1))\n",
    "sns.boxplot(x=traindf['SalePrice'])\n",
    "plt.title('Boxplot of SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef489e7",
   "metadata": {},
   "source": [
    "**SalePrice** has a strongly right-skewed distribution (skew = 1.88) with a heavy right tail, confirmed by the high kurtosis value (6.54) and the boxplot.  \n",
    "This is expected for housing prices, where most properties are sold around a typical value, and only a few are significantly more expensive.  \n",
    "The average sale price is $180,921.20, with values ranging from $34,900.00 to $755,000.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c34c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of log(SalePrice)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(np.log1p(traindf['SalePrice']), kde=True)\n",
    "plt.title('Log-Transformed SalePrice')\n",
    "plt.xlabel('Log(SalePrice)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# Calculate skewness and kurtosis\n",
    "print(f'Skewness of log(SalePrice): {np.log1p(traindf[\"SalePrice\"]).skew()}')\n",
    "print(f'Kurtosis of log(SalePrice): {np.log1p(traindf[\"SalePrice\"]).kurt()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e9a6f",
   "metadata": {},
   "source": [
    "The log-transformed SalePrice shows a nearly normal distribution.  \n",
    "Skew = 0.12 and kurtosis = 0.81 indicate that even though not perfectly normal, the shape is close enough to be treated as such.  \n",
    "\n",
    "We will use the log-transformed target for training to reduce the impact of errors on very cheap or very expensive houses, and to improve model stability.  \n",
    "This is also required by the competition, as the evaluation is based on RMSE of the log-transformed prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c28af5",
   "metadata": {},
   "source": [
    "### 2.3 Missing Values Analysis & Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecef52",
   "metadata": {},
   "source": [
    "Some of the models we will be using can deal with missing values natively, while others like those based on Neural Networks, cannot.  \n",
    "For a fair comparison, we will prepare the data so that every model has the same starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d868ce",
   "metadata": {},
   "source": [
    "#### 2.3.1 Missing values on training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce2fc48",
   "metadata": {},
   "source": [
    "Now lets look at which features have missing values and how many each have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b026c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db0fa3",
   "metadata": {},
   "source": [
    "As per the **data_description.txt** file, many of these features use NA as intented value, meaning \"None\".  \n",
    "But there are other variables where that is not the case, like **GarageYrBlt** or **MasVnrArea**.  \n",
    "\n",
    "As there are not that many features with missing values, lets explore them manually and deal with them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12425279",
   "metadata": {},
   "source": [
    "**PoolQC** is a categorical variable which describes the quality of the pool, NA is not one of the categories but given that there is no category for \"No pool\", those 1453 missing values should mean that those 1453 houses have no pool.  \n",
    "\n",
    "We can confirm this looking at how many houses have 0 pool area and checking if those entries are the same as the ones with **PoolQC** missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['PoolArea'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adf224",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['PoolArea'] == 0) & (traindf['PoolQC'].isnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b7964",
   "metadata": {},
   "source": [
    "Now lets transform empty values into a string \"None\" to avoid issues with missing values downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e655b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['PoolQC'] = traindf['PoolQC'].fillna('None')\n",
    "traindf['PoolQC'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7192a",
   "metadata": {},
   "source": [
    "**MiscFeature** is also a categorical variable with NA as intended value for None, lets compare it with **MiscVal** which represents the value of said feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['MiscVal'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d182bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['MiscVal'] == 0) & (traindf['MiscFeature'].isnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f426490",
   "metadata": {},
   "source": [
    "There seems to be 2 instances of **MiscVal** 0 more than the number of entries with **MiscFeature** as NA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f029260",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['MiscVal'] == 0) & (traindf['MiscFeature'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de936ecb",
   "metadata": {},
   "source": [
    "As there are only 2 entries, we will drop them and replace the NA values of the rest of **MiscFeature** with \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be079f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.drop(index=traindf[(traindf['MiscVal'] == 0) & (traindf['MiscFeature'].notna())].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['MiscFeature'] = traindf['MiscFeature'].fillna('None')\n",
    "traindf['MiscFeature'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c139f",
   "metadata": {},
   "source": [
    "Both **Alley** and **Fence** also use NA as None. This time there is no information to crosscheck, so we will assume all NA values are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5875ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['Alley'] = traindf['Alley'].fillna('None')\n",
    "traindf['Alley'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ede6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['Fence'] = traindf['Fence'].fillna('None')\n",
    "traindf['Fence'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35f283",
   "metadata": {},
   "source": [
    "**MasVnrType** is a categorical variable that describes the type of masonry veneer, and **MasVnrArea** is a numerical variable that measures its area in square feet.  \n",
    "For the type there is a None category but with None instead of NA. Lets check its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['MasVnrArea'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['MasVnrArea'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['MasVnrType'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0e296",
   "metadata": {},
   "source": [
    "There seems to be some type of inconsistency here, as we have 870 missing values on **MasVnrType**, 8 missing values on **MasVnrArea**, and 859 values of 0 area.  \n",
    "We should first check which rows have unexpected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47199751",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['MasVnrArea'] > 0) & (traindf['MasVnrType'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['MasVnrArea'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['MasVnrArea'] == 0) & ~(traindf['MasVnrType'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4e9e4",
   "metadata": {},
   "source": [
    "We can see 4 different cases here:  \n",
    "> 8 entries where both type and area are NA  \n",
    "> 2 entries where type is NA but where the area is 1.0, which would not make sense as that area value is too small  \n",
    "> 3 entries where type is NA but where the area has a reasonable value  \n",
    "> 2 entries where there is a valid type but the area is 0  \n",
    "\n",
    "We are going to drop the entries with both values missing, and the two with 1.0 as area, because they are only 10 entries (<1% of the total).  \n",
    "\n",
    "As for the other two cases, we are going to replace the missing values with the mode of the type from its neighborhood, and with the median area of the neighborhood.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.drop(index=traindf[(traindf['MasVnrType'].isnull()) & (traindf['MasVnrArea'].isnull())].index)\n",
    "traindf = traindf.drop(index=traindf[(traindf['MasVnrArea'] == 1.0)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfafc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean mask for those rows where MasVnrType is NaN and MasVnrArea is not 0\n",
    "mask1 = traindf['MasVnrType'].isna() & (traindf['MasVnrArea'] != 0)\n",
    "\n",
    "# Create boolean mask for those rows where MasVnrType has a valid value and MasVnrArea is 0\n",
    "mask2 = ~traindf['MasVnrType'].isna() & (traindf['MasVnrArea'] == 0)\n",
    "\n",
    "# Group by Neighborhood and get the mode of MasVnrType by Neighborhood and the median of MasVnrArea.\n",
    "MasVnrType_mode_Neighborhood = (traindf.groupby('Neighborhood')['MasVnrType'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "MasVnrArea_median_Neighborhood = traindf.groupby('Neighborhood')['MasVnrArea'].median()\n",
    "\n",
    "# Map the mode values to the original DataFrame\n",
    "traindf.loc[mask1, 'MasVnrType'] = traindf.loc[mask1, 'Neighborhood'].map(MasVnrType_mode_Neighborhood)\n",
    "traindf.loc[mask2, 'MasVnrArea'] = traindf.loc[mask2, 'Neighborhood'].map(MasVnrArea_median_Neighborhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c8db9",
   "metadata": {},
   "source": [
    "Lets check whether all the missing values remaining matches the number of properties without masonry veneer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traindf['MasVnrType'].isnull().sum())\n",
    "print((traindf['MasVnrArea'] == 0).sum())\n",
    "traindf[(traindf['MasVnrArea'] == 0) & ~(traindf['MasVnrType'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd3ca5",
   "metadata": {},
   "source": [
    "We can see there is still an entry with valid veneer type but area 0, this means that the median of that neighborhood is 0.  \n",
    "As it is only one entry, our safest approach is to just drop this one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.drop(index=traindf[(traindf['MasVnrArea'] == 0) & ~(traindf['MasVnrType'].isnull())].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4235b94",
   "metadata": {},
   "source": [
    "Now we will replace the NA values in type by 'None'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87696308",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['MasVnrType'] = traindf['MasVnrType'].fillna('None')\n",
    "traindf['MasVnrType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743a680",
   "metadata": {},
   "source": [
    "**FireplaceQu** has NA as None, and the amount should match the amount of 0 **Fireplaces**.  \n",
    "If so, we will just replace those NA with \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ab0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['FireplaceQu'].isnull()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['Fireplaces'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8283019",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['Fireplaces'] == 0) & (traindf['FireplaceQu'].isnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['FireplaceQu'] = traindf['FireplaceQu'].fillna('None')\n",
    "traindf['FireplaceQu'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798de390",
   "metadata": {},
   "source": [
    "**LotFrontage** shows the linear feet of street connected to the house.  \n",
    "As there is a big number of missing values (~17%), dropping them would not be reasonable.  \n",
    "Instead, we will replace those values by the median of the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeeda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean mask for those rows where LotFrontage is NA.\n",
    "mask = traindf['LotFrontage'].isna()\n",
    "\n",
    "# Group by Neighborhood and get the mode of LotFrontage by Neighborhood\n",
    "LotFrontage_median_Neighborhood = traindf.groupby('Neighborhood')['LotFrontage'].median()\n",
    "\n",
    "# Map the mode values to the original DataFrame\n",
    "traindf.loc[mask, 'LotFrontage'] = traindf.loc[mask, 'Neighborhood'].map(LotFrontage_median_Neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['LotFrontage'].isnull()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3fcea",
   "metadata": {},
   "source": [
    "Now we will check the garage related variables.\n",
    "We have 81 missing values on **GarageQual, GarageType, GarageFinish, GarageYrBlt, GarageExposure**.  \n",
    "All those are categorical and have NA as legitimate value for \"no garage\", except **GarageYrBlt** which is numerical (year the garage was built).  \n",
    "\n",
    "Besides those, we can see two more variables related to the garage, **GarageArea** and **GarageCars** which are numerical variables.\n",
    "\n",
    "Now we should check that those 81 missing values on each feature, they all match 81 unique entries, that at the same time should have all of them 0 in both Area and Cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53293e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['GarageCars'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ff70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf['GarageArea'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[(traindf['GarageArea'] == 0) & (traindf['GarageCars'] == 0) & (traindf['GarageQual'].isnull()) \n",
    "        & (traindf['GarageType'].isnull()) & (traindf['GarageFinish'].isnull()) \n",
    "        & (traindf['GarageCond'].isnull()) & (traindf['GarageYrBlt'].isnull())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872e12",
   "metadata": {},
   "source": [
    "After checking this we can safely replace NA values on the categorical variables with \"None\", and for **GarageYrBlt** we will replace with -1 as a placeholder to indicate there is no garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    traindf[var] = traindf[var].fillna('None')\n",
    "    print(f\"Unique values in {var}: {traindf[var].unique()}\")\n",
    "traindf['GarageYrBlt'] = traindf['GarageYrBlt'].fillna(-1)\n",
    "(traindf['GarageYrBlt'] == -1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f648c",
   "metadata": {},
   "source": [
    "As far as basement related variables, we have 11 in total:\n",
    "> **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2**, these 5 categorical features use NA as value for \"no basement\", and those are the ones that show missing values.  \n",
    "> **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF**, these 4 numerical features describe the area in square feet of the different sections of the basement, they have no missing values.  \n",
    "> **BsmtFullBath, BsmtHalfBath**, these 2 numerical features describe the amount of full and half bathrooms that there are in the basement, they have no missing values.  \n",
    "\n",
    "We expect to see that those entries with missing values in all 5 categorical features, should have 0 as value in all the numerical variables.  \n",
    "First we will check that and replace the missing values with \"None\" and then we can focus in the rest of the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "BsmtCatCols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "mask = traindf[BsmtCatCols].isnull().all(axis=1)\n",
    "\n",
    "print(\"Basement 1 area: \", \n",
    "      traindf[mask][\"BsmtFinSF1\"].value_counts())\n",
    "\n",
    "print(\"Basement 2 area: \", \n",
    "      traindf[mask][\"BsmtFinSF2\"].value_counts())\n",
    "\n",
    "print(\"Unfinished basement area: \", \n",
    "      traindf[mask][\"BsmtUnfSF\"].value_counts())\n",
    "\n",
    "print(\"Basement total area: \", \n",
    "      traindf[mask][\"TotalBsmtSF\"].value_counts())\n",
    "\n",
    "print(\"Basement full bathrooms: \", \n",
    "      traindf[mask][\"BsmtFullBath\"].value_counts())\n",
    "\n",
    "print(\"Basement half bathrooms: \", \n",
    "      traindf[mask][\"BsmtHalfBath\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e30dcb",
   "metadata": {},
   "source": [
    "With that we can safely replace in those 37 entries the missing value with \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.loc[mask, BsmtCatCols] = traindf.loc[mask, BsmtCatCols].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b626dbd",
   "metadata": {},
   "source": [
    "Lets check what missing values remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1030d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[traindf[\"BsmtExposure\"].isnull() | traindf[\"BsmtFinType2\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54b366",
   "metadata": {},
   "source": [
    "We can see that there is one entry with **BsmtFinType2** empty even though there is surface area and the other values make sense, and another entry where **BsmtExposure**'s value is missing even though the rest of the values indicate that there is an unfinished basement. Both cases seem to be missing information, not empty on purpose.  \n",
    "\n",
    "Given that it is only 2 entries, we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.drop(index=traindf[traindf[\"BsmtExposure\"].isnull() | traindf[\"BsmtFinType2\"].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414d1b1",
   "metadata": {},
   "source": [
    "As for the entry with the **Electrical** feature missing, we will drop it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41050d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.drop(index=traindf[traindf[\"Electrical\"].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a9314",
   "metadata": {},
   "source": [
    "Before we proceed lets check all missing values have been dealt with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4962354",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e355d9a",
   "metadata": {},
   "source": [
    "An empty list means there are no more missing values in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4631e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5857c1",
   "metadata": {},
   "source": [
    "We have lost in total 16 rows (~1%), but now we have a clean dataset that will not give us problems when we implement any models that cannot deal with missing values natively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1312d7",
   "metadata": {},
   "source": [
    "Lets save the cleaned dataset into a CSV file before proceeding with the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c86f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_output_path = '../data/processed/train_clean.csv'  # Output file path\n",
    "traindf.to_csv(train_clean_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9007a2",
   "metadata": {},
   "source": [
    "#### 2.3.2 Missing values on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbebfc",
   "metadata": {},
   "source": [
    "Now we will repeat the same operations with the test dataset provided.  \n",
    "But this time we cannot drop any row, as we need to make a prediction for all entries for the **Kaggle** submission.  \n",
    "\n",
    "We will start by loading the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde06cb",
   "metadata": {},
   "source": [
    "We have more features with missing values in the test dataset than in the train set, but most seem to be one or two entries.  \n",
    "\n",
    "As we cannot drop rows on the test set given that we need to make a prediction for all of them, lets first apply carefully the same transformations as with the training set and see what remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA with 'None' in every missing PoolQC that has PoolArea = 0\n",
    "mask = (testdf[\"PoolArea\"] == 0) & (testdf[\"PoolQC\"].isnull())\n",
    "testdf.loc[mask, \"PoolQC\"] = 'None'\n",
    "\n",
    "# Replace NA with 'None' in every missing MiscFeature that has MiscVal = 0\n",
    "mask = (testdf[\"MiscVal\"] == 0) & (testdf[\"MiscFeature\"].isnull())\n",
    "testdf.loc[mask, \"MiscFeature\"] = 'None'\n",
    "\n",
    "# Replace NA with 'None' in every missing Alley and Fence\n",
    "testdf['Alley'] = testdf['Alley'].fillna('None')\n",
    "testdf['Fence'] = testdf['Fence'].fillna('None')\n",
    "\n",
    "# For those with both MasVnrtype and MasVnrArea missing, we will first replace the area with the median of the neighborhood from the training set\n",
    "mask = testdf['MasVnrType'].isna() & (testdf['MasVnrArea'].isna())\n",
    "testdf.loc[mask, 'MasVnrArea'] = testdf.loc[mask, 'Neighborhood'].map(MasVnrArea_median_Neighborhood)\n",
    "# Then replace the MasVnrType with the mode of the neighborhood from the training set on those rows with a valid MasVnrArea (>0)\n",
    "mask = testdf['MasVnrType'].isna() & (testdf['MasVnrArea'] > 0)\n",
    "testdf.loc[mask, 'MasVnrType'] = testdf.loc[mask, 'Neighborhood'].map(MasVnrType_mode_Neighborhood)\n",
    "# And for those with MasVnrArea = 0 and MasVnrType missing, we will replace the type with 'None'\n",
    "mask = testdf['MasVnrType'].isna() & (testdf['MasVnrArea'] == 0)\n",
    "testdf.loc[mask, \"MasVnrType\"] = 'None'\n",
    "\n",
    "# Replace NA with 'None' in every missing FireplaceQu that has Fireplaces = 0\n",
    "mask = (testdf[\"Fireplaces\"] == 0) & (testdf[\"FireplaceQu\"].isnull())\n",
    "testdf.loc[mask, \"FireplaceQu\"] = 'None'\n",
    "\n",
    "# Replace NA with the median LotFrontage of the neighborhood from the training set\n",
    "mask = testdf['LotFrontage'].isna()\n",
    "testdf.loc[mask, 'LotFrontage'] = testdf.loc[mask, 'Neighborhood'].map(LotFrontage_median_Neighborhood)\n",
    "\n",
    "# Replace NA with 'None' in every missing categorical Garage variables, with -1 in GarageYrBlt and with 0 in GarageArea and GarageCars\n",
    "# But only for those entries where all Garage variables mean there is no garage\n",
    "mask = (\n",
    "    ((testdf['GarageArea'].isnull()) | (testdf['GarageArea'] == 0)) &\n",
    "    ((testdf['GarageCars'].isnull()) | (testdf['GarageCars'] == 0)) &\n",
    "    (testdf['GarageQual'].isnull()) &\n",
    "    (testdf['GarageType'].isnull()) &\n",
    "    (testdf['GarageFinish'].isnull()) &\n",
    "    (testdf['GarageCond'].isnull()) &\n",
    "    ((testdf['GarageYrBlt'].isnull()) | (testdf['GarageYrBlt'] == 0))\n",
    ")\n",
    "\n",
    "for var in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    testdf.loc[mask, var] = 'None'\n",
    "testdf.loc[mask, 'GarageYrBlt'] = -1\n",
    "testdf.loc[mask, 'GarageArea'] = 0\n",
    "testdf.loc[mask, 'GarageCars'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Replace NA with 'None' in every missing categorical Basement variables, and with 0 in the numerical ones\n",
    "# But only for those entries where all Basement variables mean there is no basement\n",
    "mask = (\n",
    "    ((testdf['BsmtFinSF1'].isnull()) | (testdf['BsmtFinSF1'] == 0)) &\n",
    "    ((testdf['BsmtFinSF2'].isnull()) | (testdf['BsmtFinSF2'] == 0)) &\n",
    "    ((testdf['BsmtUnfSF'].isnull()) | (testdf['BsmtUnfSF'] == 0)) &\n",
    "    ((testdf['TotalBsmtSF'].isnull()) | (testdf['TotalBsmtSF'] == 0)) &\n",
    "    ((testdf['BsmtFullBath'].isnull()) | (testdf['BsmtFullBath'] == 0)) &\n",
    "    ((testdf['BsmtHalfBath'].isnull()) | (testdf['BsmtHalfBath'] == 0)) &\n",
    "    (testdf['BsmtQual'].isnull()) &\n",
    "    (testdf['BsmtCond'].isnull()) &\n",
    "    (testdf['BsmtExposure'].isnull()) &\n",
    "    (testdf['BsmtFinType1'].isnull()) &\n",
    "    (testdf['BsmtFinType2'].isnull())\n",
    ")\n",
    "for var in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
    "    testdf.loc[mask, var] = 'None'\n",
    "for var in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n",
    "    testdf.loc[mask, var] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac163409",
   "metadata": {},
   "source": [
    "Afther those transformations the missing values remaining are the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b668c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58db1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9409dd",
   "metadata": {},
   "source": [
    "Most missing values have been cleaned, but there are a few remnants over 22 entries that we will deal with manually.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5e48b",
   "metadata": {},
   "source": [
    "Using the information provided in data_description.txt we will proceed with the following:\n",
    "\n",
    "- **MSZoning** we will replace missing values with the mode by neighborhood from the training set\n",
    "- **PoolQC** we will replace missing values with the mode by neighborhood from the training set (we previously already replaced those with 0 **PoolArea** by \"None\")\n",
    "- **Utilities** we will replace missing values with the mode by neighborhood from the training set\n",
    "- **Functional** we will replace missing values with \"Typ\" (From documentation: Assume typical unless deductions are warranted)\n",
    "- **Exterior1st** we will replace missing values with the mode by neighborhood from the training set\n",
    "- **Exterior2nd** we will replace missing values with the mode by neighborhood from the training set\n",
    "- **KitchenQual** we will replace missing values with the mode by neighborhood from the training set\n",
    "- **MiscFeature** we will replace missing values with \"Other\" category (we previously already replaced those with 0 **MiscVal** by \"None)\n",
    "- **SaleType** we will replace missing values with the mode by neighborhood from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = testdf['MSZoning'].isna()\n",
    "MSZoning_mode_Neighborhood = (traindf.groupby('Neighborhood')['MSZoning'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'MSZoning'] = testdf.loc[mask, 'Neighborhood'].map(MSZoning_mode_Neighborhood)\n",
    "\n",
    "mask = testdf['PoolQC'].isna()\n",
    "PoolQC_mode_Neighborhood = (traindf.groupby('Neighborhood')['PoolQC'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'PoolQC'] = testdf.loc[mask, 'Neighborhood'].map(PoolQC_mode_Neighborhood)\n",
    "\n",
    "mask = testdf['Utilities'].isna()\n",
    "Utilities_mode_Neighborhood = (traindf.groupby('Neighborhood')['Utilities'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'Utilities'] = testdf.loc[mask, 'Neighborhood'].map(Utilities_mode_Neighborhood)\n",
    "\n",
    "mask = testdf['Functional'].isna()\n",
    "testdf.loc[mask, 'Functional'] = \"Typ\"\n",
    "\n",
    "mask = testdf['Exterior1st'].isna()\n",
    "Exterior1st_mode_Neighborhood = (traindf.groupby('Neighborhood')['Exterior1st'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'Exterior1st'] = testdf.loc[mask, 'Neighborhood'].map(Exterior1st_mode_Neighborhood)\n",
    "\n",
    "mask = testdf['Exterior2nd'].isna()\n",
    "Exterior2nd_mode_Neighborhood = (traindf.groupby('Neighborhood')['Exterior2nd'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'Exterior2nd'] = testdf.loc[mask, 'Neighborhood'].map(Exterior2nd_mode_Neighborhood)\n",
    "\n",
    "mask = testdf['KitchenQual'].isna()\n",
    "KitchenQual_mode_Neighborhood = (traindf.groupby('Neighborhood')['KitchenQual'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'KitchenQual'] = testdf.loc[mask, 'Neighborhood'].map(KitchenQual_mode_Neighborhood)\n",
    "\n",
    "mask = testdf['MiscFeature'].isna()\n",
    "testdf.loc[mask, 'MiscFeature'] = \"Other\"\n",
    "\n",
    "mask = testdf['SaleType'].isna()\n",
    "SaleType_mode_Neighborhood = (traindf.groupby('Neighborhood')['SaleType'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "testdf.loc[mask, 'SaleType'] = testdf.loc[mask, 'Neighborhood'].map(SaleType_mode_Neighborhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762879fc",
   "metadata": {},
   "source": [
    "All we should have left are the Basement and Garage related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0747d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659033a",
   "metadata": {},
   "source": [
    "They are only 9 entries, lets explore them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf[testdf.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040b625",
   "metadata": {},
   "source": [
    "We can see there is one entry where even though there is **GarageType** defined, the rest of the information related to the garage is missing, so we are going to assume there is no garage on the property and the type was a data error.  \n",
    "\n",
    "The rest of the entries show enough information about basement and garage, so we will replace the missing parts using the mode by neighborhood on the categorical variables and the median on the numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9727c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets deal with the row where we are assuming there is no garage\n",
    "row_label = testdf[testdf[\"GarageCars\"].isnull()].index[0]\n",
    "\n",
    "# Now lets change the rest of the garage variables\n",
    "for var in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    testdf.loc[row_label, var] = 'None'\n",
    "testdf.loc[row_label, \"GarageArea\"] = 0.0\n",
    "testdf.loc[row_label, \"GarageCars\"] = 0.0\n",
    "testdf.loc[row_label, \"GarageYrBlt\"] = -1\n",
    "\n",
    "# Now for the rest, lets replace the missing values of the categorical variables with the mode of the neighborhood from the training set\n",
    "for var in ['GarageFinish', 'GarageQual', 'GarageCond', 'BsmtExposure', 'BsmtQual', 'BsmtCond']:\n",
    "    mask = testdf[var].isna()\n",
    "    mode = (traindf.groupby('Neighborhood')[var].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'None'))\n",
    "    testdf.loc[mask, var] = testdf.loc[mask, 'Neighborhood'].map(mode)\n",
    "\n",
    "# And for GarageYrBlt, the only numerical variable, we will replace it with the median of the neighborhood from the training set\n",
    "GarageYrBlt_median_Neighborhood = traindf.groupby('Neighborhood')['GarageYrBlt'].median()\n",
    "row_label = testdf[testdf[\"GarageYrBlt\"].isnull()].index[0]\n",
    "neighborhood = testdf.loc[row_label, 'Neighborhood']\n",
    "testdf.loc[row_label, 'GarageYrBlt'] = GarageYrBlt_median_Neighborhood[neighborhood]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac1b42",
   "metadata": {},
   "source": [
    "Let's do a final check to make sure all missing values have been dealt with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.isnull().sum().sort_values(ascending=False)[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31de63c",
   "metadata": {},
   "source": [
    "And now lets save the cleaned test dataset to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9548c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_output_path = '../data/processed/test_clean.csv'  # Output file path\n",
    "testdf.to_csv(test_clean_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4a65b",
   "metadata": {},
   "source": [
    "### 2.4 Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78453b",
   "metadata": {},
   "source": [
    "We will first check how many numerical and categorical features we have.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categorical columns:\\n{traindf.select_dtypes(include=['object']).columns}\\n\")\n",
    "print(f\"Numerical columns:\\n{traindf.select_dtypes(include=['int64', 'float64']).columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8da3d",
   "metadata": {},
   "source": [
    "Taking a look at this, we can see that we cannot directly separate them by type, as there are some categorical features stored as numbers.  \n",
    "We are going to manually check the features information on **data_description.txt** and adjust these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
    "    'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "    'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "    'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n",
    "    'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType',\n",
    "    'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n",
    "    'MiscFeature', 'MoSold', 'SaleType', 'SaleCondition'\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "    'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "    'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "    'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n",
    "    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "    'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of categorical features: {len(cat_features)}\")\n",
    "print(f\"Number of numerical features: {len(num_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b4796",
   "metadata": {},
   "source": [
    "#### 2.4.1 Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d309a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_features:\n",
    "    print(f\"{col}:\")\n",
    "    # Histogram with KDE\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(traindf[col], kde=True)\n",
    "    plt.title(f'{col} distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Value count')\n",
    "    plt.show()\n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(8, 1))\n",
    "    sns.boxplot(x=traindf[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "    # Scatter plot against SalePrice\n",
    "    sns.scatterplot(data=traindf, x=col, y=traindf['SalePrice'])\n",
    "    plt.title(f\"{col} vs. SalePrice\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbb42f",
   "metadata": {},
   "source": [
    "#### 2.4.2 Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features:\n",
    "    print(f\"{col}:\")\n",
    "    vc = traindf[col].value_counts()\n",
    "    print(f'- {len(vc)} unique categories')\n",
    "    # Count plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(data=traindf, x=col, order=vc.index)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Value count per category of {col}')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Value count')\n",
    "    plt.show()\n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x=col, y=traindf['SalePrice'], data=traindf)\n",
    "    plt.title(f\"SalePrice by {col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302764f3",
   "metadata": {},
   "source": [
    "### 2.5 Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = traindf[num_features + [\"SalePrice\"]].corr()\n",
    "plt.figure(figsize=(15, 13))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", annot=True, fmt=\".2f\", mask=np.triu(np.ones_like(corr, dtype=bool)))\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf449a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = corr['SalePrice'].drop('SalePrice').abs().sort_values(ascending=False).head(5).index.tolist()\n",
    "sns.pairplot(traindf[top_corr + ['SalePrice']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = mutual_info_regression(traindf[num_features], traindf['SalePrice'])\n",
    "mi_scores = pd.Series(mi_scores, index=num_features).sort_values(ascending=False)\n",
    "\n",
    "sns.barplot(x=mi_scores.values, y=mi_scores.index)\n",
    "plt.title(\"Mutual Information Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842157f",
   "metadata": {},
   "source": [
    "## 3. Feature engineering  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba0d69",
   "metadata": {},
   "source": [
    "### 3.1 New features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d1a46",
   "metadata": {},
   "source": [
    "Most of the models well be using (such as tree-based and deep learning models) are capable of learning interactions between features. However, explicitly providing these interactions through feature engineering can improve learning efficiency, reduce the risk of important relationships being missed, and often lead to better performance with less data.  \n",
    "\n",
    "With this in mind, we will manually create new features based on available data that capture potentially useful relationships relevant to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3e8ff",
   "metadata": {},
   "source": [
    "We are going to create the following on both sets of data:\n",
    "- **TotalBathrooms**: Including basement ones\n",
    "- **TotalSF**: Both floors plus basement\n",
    "- **FinishedSF**: Total livable area (excluding unfinished basement)\n",
    "- **Has2ndFloor**: Yes/No\n",
    "- **HasBasement**: Yes/No\n",
    "- **HasGarage**: Yes/No\n",
    "- **HasPool**: Yes/No\n",
    "- **HouseAge**: Years from when it was build till sale\n",
    "- **GarageAge**: Years from when it was build till sale\n",
    "- **RemodelAge**: Years from last remodelation till sale\n",
    "- **WasRemodeled**: Yes/No\n",
    "- **QualityIndex**: Ratio expressing overall quality and condition\n",
    "- **LotRatio**: Ratio expressing relative house to land size (area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd914eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[\"TotalBathrooms\"] = traindf[\"FullBath\"] + (0.5 * traindf[\"HalfBath\"]) + traindf[\"BsmtFullBath\"] + (0.5 * traindf[\"BsmtHalfBath\"])\n",
    "testdf[\"TotalBathrooms\"] = testdf[\"FullBath\"] + (0.5 * testdf[\"HalfBath\"]) + testdf[\"BsmtFullBath\"] + (0.5 * testdf[\"BsmtHalfBath\"])\n",
    "\n",
    "traindf[\"TotalSF\"] = traindf[\"TotalBsmtSF\"] + traindf[\"1stFlrSF\"] + traindf[\"2ndFlrSF\"]\n",
    "testdf[\"TotalSF\"] = testdf[\"TotalBsmtSF\"] + testdf[\"1stFlrSF\"] + testdf[\"2ndFlrSF\"]\n",
    "\n",
    "traindf[\"FinishedSF\"] = traindf[\"BsmtFinSF1\"] + traindf[\"BsmtFinSF2\"] + traindf[\"1stFlrSF\"] + traindf[\"2ndFlrSF\"]\n",
    "testdf[\"FinishedSF\"] = testdf[\"BsmtFinSF1\"] + testdf[\"BsmtFinSF2\"] + testdf[\"1stFlrSF\"] + testdf[\"2ndFlrSF\"]\n",
    "\n",
    "traindf[\"Has2ndFloor\"] = (traindf[\"2ndFlrSF\"] > 0).astype(int)\n",
    "testdf[\"Has2ndFloor\"] = (testdf[\"2ndFlrSF\"] > 0).astype(int)\n",
    "\n",
    "traindf[\"HasBasement\"] = (traindf[\"TotalBsmtSF\"] > 0).astype(int)\n",
    "testdf[\"HasBasement\"] = (testdf[\"TotalBsmtSF\"] > 0).astype(int)\n",
    "\n",
    "traindf[\"HasGarage\"] = (traindf[\"GarageArea\"] > 0).astype(int)\n",
    "testdf[\"HasGarage\"] = (testdf[\"GarageArea\"] > 0).astype(int)\n",
    "\n",
    "traindf[\"HasPool\"] = (traindf[\"PoolArea\"] > 0).astype(int)\n",
    "testdf[\"HasPool\"] = (testdf[\"PoolArea\"] > 0).astype(int)\n",
    "\n",
    "traindf[\"HouseAge\"] = traindf[\"YrSold\"] - traindf[\"YearBuilt\"]\n",
    "testdf[\"HouseAge\"] = testdf[\"YrSold\"] - testdf[\"YearBuilt\"]\n",
    "\n",
    "traindf[\"GarageAge\"] = traindf[\"YrSold\"] - traindf[\"GarageYrBlt\"]\n",
    "testdf[\"GarageAge\"] = testdf[\"YrSold\"] - testdf[\"GarageYrBlt\"]\n",
    "\n",
    "traindf[\"RemodelAge\"] = traindf[\"YrSold\"] - traindf[\"YearRemodAdd\"]\n",
    "testdf[\"RemodelAge\"] = testdf[\"YrSold\"] - testdf[\"YearRemodAdd\"]\n",
    "\n",
    "traindf[\"WasRemodel\"] = (traindf[\"YearRemodAdd\"] != traindf[\"YearBuilt\"]).astype(int)\n",
    "testdf[\"WasRemodel\"] = (testdf[\"YearRemodAdd\"] != testdf[\"YearBuilt\"]).astype(int)\n",
    "\n",
    "traindf[\"QualityIndex\"] = traindf[\"OverallQual\"] * traindf[\"OverallCond\"]\n",
    "testdf[\"QualityIndex\"] = testdf[\"OverallQual\"] * testdf[\"OverallCond\"]\n",
    "\n",
    "traindf[\"LotRatio\"] = traindf[\"GrLivArea\"] / traindf[\"LotArea\"]\n",
    "testdf[\"LotRatio\"] = testdf[\"GrLivArea\"] / testdf[\"LotArea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1cec6",
   "metadata": {},
   "source": [
    "### 3.2 Domain-driven transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ee872",
   "metadata": {},
   "source": [
    "### 3.3 Drop unneeded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b898fce",
   "metadata": {},
   "source": [
    "Besides this, we will drop the **Id** column in both datasets, as it offers no useful information for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.drop('Id', axis=1, inplace=True)\n",
    "testdf.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820606a",
   "metadata": {},
   "source": [
    "And we will save both engineered datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng_output_path = '../data/processed/train_engineered.csv'  # Output file path\n",
    "traindf.to_csv(train_eng_output_path, index=False)\n",
    "\n",
    "test_eng_output_path = '../data/processed/test_engineered.csv'  # Output file path\n",
    "testdf.to_csv(test_eng_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ccb13",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd635f6",
   "metadata": {},
   "source": [
    "We are working with tabular data that includes both numerical and categorical features.  \n",
    "Depending on the model type, specific preprocessing (like encoding categorical variables or normalizing numerical features) may be necessary.  \n",
    "\n",
    "Since each model handles input data differently (some may not benefit from certain transformations or may even be incompatible with them), we will normaliza and/or encode variables when needed, separately for each model.\n",
    "\n",
    "However, to ensure a fair comparison, we will apply some common transformations beforehand to create our shared \"starting point\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e910313",
   "metadata": {},
   "source": [
    "### 4.1 Log-transform target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937352b",
   "metadata": {},
   "source": [
    "Let's transform our target variable **SalePrice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fce5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[\"SalePrice\"] = np.log1p(traindf[\"SalePrice\"]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663271fd",
   "metadata": {},
   "source": [
    "### 4.2 Sanitize variables data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f927a",
   "metadata": {},
   "source": [
    "To avoid mixed data types problems, specially with the neural networks models, we will sanitize the data by making sure all data has the following format:  \n",
    "- Numerical variables: float32\n",
    "- Categorical variables: String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f6bac",
   "metadata": {},
   "source": [
    "First we will update our lists containing which features are numerical and which categorical, to include the changes made on the **Feature engineering** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
    "    'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "    'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "    'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n",
    "    'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType',\n",
    "    'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n",
    "    'MiscFeature', 'MoSold', 'SaleType', 'SaleCondition', 'Has2ndFloor', 'HasBasement',\n",
    "    'HasGarage', 'HasPool', 'WasRemodel'\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "    'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "    'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "    'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n",
    "    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "    'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold', 'TotalBathrooms', 'TotalSF',\n",
    "    'FinishedSF', 'HouseAge', 'GarageAge', 'RemodelAge', 'QualityIndex', 'LotRatio'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of categorical features: {len(cat_features)}\")\n",
    "print(f\"Number of numerical features: {len(num_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81166ba5",
   "metadata": {},
   "source": [
    "And now we will loop over the features changing types where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_features:\n",
    "    traindf[cat] = traindf[cat].astype(str)\n",
    "    testdf[cat] = testdf[cat].astype(str)\n",
    "\n",
    "for num in num_features:\n",
    "    traindf[num] = traindf[num].astype(np.float32)\n",
    "    testdf[num] = testdf[num].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965a3a6",
   "metadata": {},
   "source": [
    "### 4.3 Train/dev/holdout data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55802a08",
   "metadata": {},
   "source": [
    "Our dataset is relatively small. As mentioned earlier, while a \"test\" set has been provided, it does not include the target variable or results.  \n",
    "This test set will be used only as unseen data for making final predictionsit will not be used for training or evaluation during model development.  \n",
    "\n",
    "Given this, we need to split our available training data. With approximately 1,500 examples, we will use a 70/20/10 split:\n",
    "\n",
    "- 70% for training (**train set**)\n",
    "- 20% for evaluation and fine-tuning during development (**dev set**)\n",
    "- 10% for final evaluation to assess overfitting and compare models (**holdout set**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = traindf[\"SalePrice\"]\n",
    "X = traindf.drop(\"SalePrice\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eed75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=33)\n",
    "X_dev, X_hold, Y_dev, Y_hold = train_test_split(X_temp, Y_temp, test_size=1/3, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a146d",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bda31f",
   "metadata": {},
   "source": [
    "### 5.1 Baseline model (Ridge regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71213709",
   "metadata": {},
   "source": [
    "We will use Scikit-Learn to build a simple linear model.  \n",
    "\n",
    "In this case we will use Ridge Regression instead of linear regression due to the high number of features (the regularization could help avoid overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd401c",
   "metadata": {},
   "source": [
    "First we will define the preprocessor that will encode the categorical variables using one-hot encoding.  \n",
    "The numerical variables will not be scaled or normalized as it is not necessary for this model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f089e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "        # numerical columns are passed through unchanged\n",
    "        ('num', 'passthrough', num_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a5830",
   "metadata": {},
   "source": [
    "We will build the model, and fit it to our training data.  \n",
    "Because this is just a baseline model used for comparison, we will leave the alpha parameter as its default value (1.0), and will not try to improve it any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5fd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_model = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", Ridge(alpha=1.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "Ridge_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd6cb5",
   "metadata": {},
   "source": [
    "Now lets evaluate the model on our dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7135fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on dev set\n",
    "Y_pred = Ridge_model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a594b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_dev_rmse_log = root_mean_squared_error(Y_dev, Y_pred)\n",
    "Ridge_dev_rmse_dollars = root_mean_squared_error(np.expm1(Y_dev), np.expm1(Y_pred))\n",
    "print(f\"Ridge RMSE (log scale): {Ridge_dev_rmse_log:.4f}\")\n",
    "print(f\"Random Forest RMSE (dollars): {Ridge_dev_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16d0f6",
   "metadata": {},
   "source": [
    "And also on our holdout set for comparison purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on hold set\n",
    "Y_pred = Ridge_model.predict(X_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aed210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_hold_rmse_log = root_mean_squared_error(Y_hold, Y_pred)\n",
    "Ridge_hold_rmse_dollars = root_mean_squared_error(np.expm1(Y_hold), np.expm1(Y_pred))\n",
    "print(f\"Ridge RMSE (log scale): {Ridge_hold_rmse_log:.4f}\")\n",
    "print(f\"Ridge RMSE (dollars): {Ridge_hold_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4daf4a0",
   "metadata": {},
   "source": [
    "Finally, we will store the results for the model comparison later on, and save our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd39af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = []\n",
    "\n",
    "model_summary.append({\n",
    "    'model': 'Ridge Regression',\n",
    "    'dev_rmse_log': Ridge_dev_rmse_log,\n",
    "    'dev_rmse_dollars': Ridge_dev_rmse_dollars,\n",
    "    'holdout_rmse_log': Ridge_hold_rmse_log,\n",
    "    'holdout_rmse_dollars': Ridge_hold_rmse_dollars\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943881f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(Ridge_model, '../models/ridge_model.pkl')\n",
    "\n",
    "del Ridge_model\n",
    "\n",
    "# Ridge_model = joblib.load('../models/ridge_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c764ac",
   "metadata": {},
   "source": [
    "### 5.2 Random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e62e5",
   "metadata": {},
   "source": [
    "We will use TensorFLow Decision Forests to build our Random Forest model.  \n",
    "\n",
    "TFDF can handle internally the encoding of categorical features, and as such we will be using the engineered (and previously cleaned) datasets directly with no preprocessing.\n",
    "\n",
    "We will begin with a basic Random Forest model with reasonable hyperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf205b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = tfdf.keras.RandomForestModel(\n",
    "    task=tfdf.keras.Task.REGRESSION,        # Define the task (Regression)\n",
    "    num_trees=300,                          # Number of trees in the forest\n",
    "    max_depth=10,                           # Maximum depth of trees\n",
    "    min_examples=5,                         # Minimum number of examples per leaf node\n",
    "    categorical_algorithm=\"CART\",           # Algorithm for handling categorical features\n",
    "    compute_oob_variable_importances=True,  # Compute out-of-bag variable importances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model.fit(tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "    pd.concat([X_train, Y_train], axis=1), \n",
    "    task=tfdf.keras.Task.REGRESSION, label=\"SalePrice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03891fb",
   "metadata": {},
   "source": [
    "Lets check the out-of-bag performance metrics (train set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_inspector = RF_model.make_inspector()\n",
    "print(RF_inspector.evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b5e2f",
   "metadata": {},
   "source": [
    "And finally lets make the predictions with the dev set and evaluate how our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = RF_model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_dev_rmse_log = root_mean_squared_error(Y_dev, Y_pred)\n",
    "RF_dev_rmse_dollars = root_mean_squared_error(np.expm1(Y_dev), np.expm1(Y_pred))\n",
    "\n",
    "print(f\"Random Forest RMSE (log scale): {RF_dev_rmse_log:.4f}\")\n",
    "print(f\"Random Forest RMSE (dollars): {RF_dev_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f8164",
   "metadata": {},
   "source": [
    "We can see already a substantial improvement over our baseline model in the dev set:  \n",
    "- Ridge Holdout RMSE (log scale): 0.1641\n",
    "- RandomForest Holdout RMSE (log scale): 0.1383  \n",
    "\n",
    "But before evaluating on the holdout set, lets finetune our Random Forest model and see how much can it be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del RF_model # Free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the Random Forest model with different hyperparameters\n",
    "\n",
    "def evaluate_rf_model(num_trees, max_depth, min_examples):\n",
    "    RF_model = tfdf.keras.RandomForestModel(\n",
    "        task=tfdf.keras.Task.REGRESSION,             # Define the task (Regression)\n",
    "        num_trees=num_trees,                         # Number of trees in the forest\n",
    "        max_depth=max_depth,                         # Maximum depth of trees\n",
    "        min_examples=min_examples,                   # Minimum number of examples per leaf node\n",
    "        categorical_algorithm=\"CART\",                # Algorithm for handling categorical features\n",
    "        compute_oob_variable_importances=False,      # Compute out-of-bag variable importances\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    RF_model.fit(tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "        pd.concat([X_train, Y_train], axis=1), \n",
    "        task=tfdf.keras.Task.REGRESSION, label=\"SalePrice\"), verbose=0)\n",
    "\n",
    "    # Predict on dev set and calculate RMSE\n",
    "    Y_pred = RF_model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(X_dev))\n",
    "    rmse = root_mean_squared_error(Y_dev, Y_pred)\n",
    "\n",
    "    # Clean up the model to free memory\n",
    "    del RF_model\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the results\n",
    "RF_tuning = []\n",
    "\n",
    "# Loop through different hyperparameters\n",
    "for trees in [100, 300, 500]:\n",
    "    for depth in [8, 10, 12]:\n",
    "        for minex in [2, 5, 10]:\n",
    "            # Evaluate model and get RMSE\n",
    "            rmse = evaluate_rf_model(trees, depth, minex)\n",
    "            \n",
    "            # Append the results to the list\n",
    "            RF_tuning.append({\n",
    "                \"num_trees\": trees,\n",
    "                \"max_depth\": depth,\n",
    "                \"min_examples\": minex,\n",
    "                \"rmse\": rmse\n",
    "            })\n",
    "\n",
    "# Convert the results to a DataFrame and sort by RMSE\n",
    "RF_tuning_df = pd.DataFrame(RF_tuning)\n",
    "RF_tuning_df = RF_tuning_df.sort_values(by=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2afd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(RF_tuning_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc5237b",
   "metadata": {},
   "source": [
    "Lets now fit our final Random Forest model with the best performing set of hyperparameters:  \n",
    "- num_trees = 500\n",
    "- max_depth = 12\n",
    "- min_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34324e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = tfdf.keras.RandomForestModel(\n",
    "    task=tfdf.keras.Task.REGRESSION,        # Define the task (Regression)\n",
    "    num_trees=500,                          # Number of trees in the forest\n",
    "    max_depth=12,                           # Maximum depth of trees\n",
    "    min_examples=5,                         # Minimum number of examples per leaf node\n",
    "    categorical_algorithm=\"CART\",           # Algorithm for handling categorical features\n",
    "    compute_oob_variable_importances=True,  # Compute out-of-bag variable importances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31287c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model.fit(tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "    pd.concat([X_train, Y_train], axis=1), \n",
    "    task=tfdf.keras.Task.REGRESSION, label=\"SalePrice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc495a",
   "metadata": {},
   "source": [
    "Lets check the performance on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aedee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_inspector = RF_model.make_inspector()\n",
    "print(RF_inspector.evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5ed09",
   "metadata": {},
   "source": [
    "And evaluate on both the dev and the holdout sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = RF_model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_dev_rmse_log = root_mean_squared_error(Y_dev, Y_pred)\n",
    "RF_dev_rmse_dollars = root_mean_squared_error(np.expm1(Y_dev), np.expm1(Y_pred))\n",
    "\n",
    "print(f\"Random Forest RMSE (log scale): {RF_dev_rmse_log:.4f}\")\n",
    "print(f\"Random Forest RMSE (dollars): {RF_dev_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = RF_model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(X_hold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hold_rmse_log = root_mean_squared_error(Y_hold, Y_pred)\n",
    "RF_hold_rmse_dollars = root_mean_squared_error(np.expm1(Y_hold), np.expm1(Y_pred))\n",
    "\n",
    "print(f\"Random Forest RMSE (log scale): {RF_hold_rmse_log:.4f}\")\n",
    "print(f\"Random Forest RMSE (dollars): {RF_hold_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35118afe",
   "metadata": {},
   "source": [
    "Performance has not worsened on the holdout set (it got even better but that is probably just due to the small holdout sample), which means weare probably not overfitting the data.  \n",
    "\n",
    "As the final step, we will store the performance metrics for the final comparison, and save our model so we can load it up again anytime without wasting time training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary.append({\n",
    "    'model': 'Random Forest',\n",
    "    'dev_rmse_log': RF_dev_rmse_log,\n",
    "    'dev_rmse_dollars': RF_dev_rmse_dollars,\n",
    "    'holdout_rmse_log': RF_hold_rmse_log,\n",
    "    'holdout_rmse_dollars': RF_hold_rmse_dollars\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5434e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model.save(\"../models/rf_model\")\t\n",
    "\n",
    "del RF_model\n",
    "\n",
    "# Load the model\n",
    "# RF_model = tfdf.keras.models.load_model(\"../models/rf_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd0cd4",
   "metadata": {},
   "source": [
    "### 5.3 Catboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be888e",
   "metadata": {},
   "source": [
    "The next model we are going to train is going to be a gradient boosting model, specifically the regressor model of the CatBoost framework.  \n",
    "\n",
    "Gradient boosting models work exceptionally well with tabular data, like other tree-based models, and for a dataset such as this one, small in size and with a relatively big number of categorical features, CatBoost models outperform most models even without much finetuning.  \n",
    "\n",
    "Given the type of task and data, this is probably going to be our best performing model, but let's not jump ahead of ourselves and proceed as usual, and we will see what happens by the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf28313",
   "metadata": {},
   "source": [
    "Before training any models, we need to transform our data to the appropiate format native to this framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pools (CatBoost's data structure)\n",
    "train_pool = Pool(data=X_train, label=Y_train, cat_features=cat_features)\n",
    "dev_pool = Pool(data=X_dev, label=Y_dev, cat_features=cat_features)\n",
    "hold_pool = Pool(data=X_hold, label=Y_hold, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc609a45",
   "metadata": {},
   "source": [
    "Now we will train a basic model with reasonable hyperparams to check that everything works before trying to finetune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ec0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create the model\n",
    "def create_CB_model(lr, depth, l2reg, bag_temp, random_seed=33, verbose=100):\n",
    "    CB_model = CatBoostRegressor(\n",
    "        iterations=3000,\n",
    "        early_stopping_rounds=50,\n",
    "        learning_rate=lr,\n",
    "        depth=depth,\n",
    "        l2_leaf_reg=l2reg,\n",
    "        bagging_temperature=bag_temp,\n",
    "        eval_metric='RMSE',\n",
    "        random_seed=random_seed,\n",
    "        verbose=verbose  # Print progress after how many iterations\n",
    "    )\n",
    "    return CB_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic model\n",
    "CB_model = create_CB_model(0.1, 6, 3, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481134f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "CB_model.fit(train_pool, eval_set=dev_pool, use_best_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72141fc6",
   "metadata": {},
   "source": [
    "And now we can check the RMSE obtained with the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ebb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_model.get_best_score()['validation']['RMSE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340ba98",
   "metadata": {},
   "source": [
    "We can already see an improvement against our baseline, but with worse performance than our final Random Forest model.  \n",
    "But just as we did withe the latter, we will finetune its hyperparams to try to boost its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6ba67",
   "metadata": {},
   "source": [
    "First we will create the list of parameters to iterate over.  \n",
    "Note that we wont be trying different numbers of iterations, because we set a farly big number of iterations (3000) with early stopping, so it will automatically stop if and when it plateaus or start overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "del CB_model # Delete the basic model we just created\n",
    "\n",
    "lr_values = [0.01, 0.03, 0.1, 0.2]\n",
    "depth_values = [4, 6, 8, 10]\n",
    "L2_values = [1, 3, 5, 10]\n",
    "bag_temp_values = [0, 0.5, 1.0, 5.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03de50",
   "metadata": {},
   "source": [
    "And then we will use a nested loop to try all the combinations while storing the performance metrics for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_results = []\n",
    "\n",
    "for lr in lr_values:\n",
    "    for depth in depth_values:\n",
    "        for l2reg in L2_values:\n",
    "            for bag_temp in bag_temp_values:\n",
    "                # Define and train model\n",
    "                CB_model = create_CB_model(lr, depth, l2reg, bag_temp, verbose=0)\n",
    "                \n",
    "                # Fit the model\n",
    "                CB_model.fit(train_pool, eval_set=dev_pool, use_best_model=True)\n",
    "\n",
    "                # Get rmse value\n",
    "                rmse = CB_model.get_best_score()['validation']['RMSE']\n",
    "\n",
    "                # Store results\n",
    "                catboost_results.append({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"depth\": depth,\n",
    "                    \"L2_leaf_reg\": l2reg,\n",
    "                    \"bagging_temperature\": bag_temp,\n",
    "                    \"rmse\": rmse})\n",
    "                \n",
    "                # Clean up memory\n",
    "                del CB_model\n",
    "\n",
    "# Convert to DataFrame\n",
    "catboost_results_df = pd.DataFrame(catboost_results)\n",
    "\n",
    "# Sort by best RMSE\n",
    "catboost_results_df = catboost_results_df.sort_values(by=\"rmse\")\n",
    "\n",
    "# Show best settings:\n",
    "print(\"\\nTop 10 settings by RMSE:\")\n",
    "catboost_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2ec64",
   "metadata": {},
   "source": [
    "Given this, our chosen hyperparams are the following:  \n",
    "- learning_rate = 0.03\n",
    "- depth = 6\n",
    "- L2_leaf_reg = 1.0\n",
    "- bagging_temperature = 1.0 (default, as results showed no change)\n",
    "\n",
    "Now we will train this with different random_seeds to check for stability, to ensure that our performance is not due to a lucky match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc49fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [11, 33, 42, 55, 64]\n",
    "\n",
    "for seed in random_seeds:\n",
    "    # Define and train model\n",
    "    CB_model = create_CB_model(0.03, 6, 1, 1.0, random_seed=seed, verbose=0)\n",
    "\n",
    "    # Fit the model\n",
    "    CB_model.fit(train_pool, eval_set=dev_pool, use_best_model=True)\n",
    "\n",
    "    # Get rmse value\n",
    "    rmse = CB_model.get_best_score()['validation']['RMSE']\n",
    "\n",
    "    print(f\"Random seed {seed}: RMSE = {rmse:.4f}\")\n",
    "    # Clean up\n",
    "    del CB_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b040a1d",
   "metadata": {},
   "source": [
    "The performance is roughly similar, which means we can proceed forward with building our final CatBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model\n",
    "CB_model = create_CB_model(0.03, 6, 1, 1.0, verbose=100)\n",
    "\n",
    "# Fit the model\n",
    "CB_model.fit(train_pool, eval_set=dev_pool, use_best_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e038dd5",
   "metadata": {},
   "source": [
    "And now we can evaluate the final performance on both dev and holdout sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c322778",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = CB_model.predict(dev_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51cfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_dev_rmse_log = root_mean_squared_error(Y_dev, Y_pred)\n",
    "CB_dev_rmse_dollars = root_mean_squared_error(np.expm1(Y_dev), np.expm1(Y_pred))\n",
    "print(f\"CatBoost RMSE (log scale): {CB_dev_rmse_log:.4f}\")\n",
    "print(f\"CatBoost RMSE (dollars): {CB_dev_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e484e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = CB_model.predict(hold_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14def602",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_hold_rmse_log = root_mean_squared_error(Y_hold, Y_pred)\n",
    "CB_hold_rmse_dollars = root_mean_squared_error(np.expm1(Y_hold), np.expm1(Y_pred))\n",
    "print(f\"CatBoost RMSE (log scale): {CB_hold_rmse_log:.4f}\")\n",
    "print(f\"CatBoost RMSE (dollars): {CB_hold_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e85f1b",
   "metadata": {},
   "source": [
    "And finally, we will store the results and export our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e49859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary.append({\n",
    "    'model': 'CatBoost model',\n",
    "    'dev_rmse_log': CB_dev_rmse_log,\n",
    "    'dev_rmse_dollars': CB_dev_rmse_dollars,\n",
    "    'holdout_rmse_log': CB_hold_rmse_log,\n",
    "    'holdout_rmse_dollars': CB_hold_rmse_dollars\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5be175",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_model.save_model(\"../models/catboost_model.cbm\")\n",
    "\n",
    "del CB_model\n",
    "\n",
    "# Load the model\n",
    "# CB_model = CatBoostRegressor()\n",
    "# CB_model.load_model(\"../models/catboost_model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b17f5",
   "metadata": {},
   "source": [
    "### 5.4 MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480312b6",
   "metadata": {},
   "source": [
    "For our next model, we will use a neural network model, with a basic MLP architecture.\n",
    "\n",
    "First the model will normalize all numerical features, and it will encode the categorical ones.  \n",
    "Given the amount of categorical variables and their cardinality (most under 10, but a few of them go between 15 and 20, with **Neighborhood** having 25 categories), maybe embedding would be a better option than one-hot encoding, but given that the cardinality in general is not that high, and that this model is probably going to be outperformed by others, we will keep it simpler with one-hot.  \n",
    "\n",
    "\n",
    "After preprocessing the input it will then use 2 fully connected layers, with 128 and 64 hidden units, and with 'Relu' as activation function.  \n",
    "\n",
    "As output layer given that this is a regression task, it will have a fully connected layer with one unit and no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_instance(num_features, cat_features, X_train):\n",
    "    # Input layer\n",
    "    # This will create a dictionary which contains an input layer per feature, to help us preprocess them individually\n",
    "    inputs = {}\n",
    "    for name in num_features + cat_features:\n",
    "        inputs[name] = Input(shape=(1,), name=name, dtype='float32' if name in num_features else 'string')\n",
    "\n",
    "    # Preprocessing for numerical\n",
    "    norm_layers = {}\n",
    "    for feature in num_features:\n",
    "        norm = Normalization() # Create a Normalization layer\n",
    "        norm.adapt(X_train[feature].values.reshape(-1, 1))  # Gets the mean and variance of the training data, reshape needed to transform (n_samples,) to (n_samples, 1)\n",
    "        norm_layers[feature] = norm(inputs[feature]) # Apply normalization to the input feature\n",
    "\n",
    "    # Preprocessing for categorical\n",
    "    cat_layers = {}\n",
    "    for feature in cat_features:\n",
    "        lookup = StringLookup(output_mode='int') # Layer to map strings to integers\n",
    "        lookup.adapt(X_train[feature].values) # Learns all unique values (categories) of the feature to map them to integers\n",
    "        \n",
    "        vocab_size = lookup.vocabulary_size() # Number of unique values (categories) in the feature + 1 for the UNK token\n",
    "        encoding = CategoryEncoding(output_mode='one_hot', num_tokens=vocab_size) # Layer to convert the integer encoded feature to one-hot encoding\n",
    "        \n",
    "        int_encoded = lookup(inputs[feature]) # Apply the lookup layer to the features\n",
    "        one_hot_encoded = encoding(int_encoded) # Apply the encoding layer to the integer encoded feature\n",
    "        cat_layers[feature] = one_hot_encoded # Store the one-hot encoded tensor to the dict\n",
    "\n",
    "    # Combine all features\n",
    "    all_features = list(norm_layers.values()) + list(cat_layers.values())\n",
    "    x = Concatenate()(all_features)\n",
    "\n",
    "    # MLP\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537545c4",
   "metadata": {},
   "source": [
    "We will train the model once to make sure everything is working, and for that first of all we need to create the basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172446b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP_instance(num_features, cat_features, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b6950",
   "metadata": {},
   "source": [
    "Now let's configure it to use Adam as optimization algorithm, using MSE as loss function and we will use RMSE as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf019f",
   "metadata": {},
   "source": [
    "We need to transform our training data to the format TF expects (dict of column name - array of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_inputs = {\n",
    "    col: X_train[col].values for col in num_features + cat_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ea329",
   "metadata": {},
   "source": [
    "And finally lets train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5531b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.fit(X_train_inputs, Y_train.values, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352308c",
   "metadata": {},
   "source": [
    "We will apply the same transformation to the dev set for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_inputs = {\n",
    "    col: X_dev[col].values for col in num_features + cat_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, rmse = MLP_model.evaluate(X_dev_inputs, Y_dev.values)\n",
    "print(f\"MLP RMSE (log scale): {rmse:.4f}, loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e162957",
   "metadata": {},
   "source": [
    "The model is working fine but with poor performance compared to even our baseline model.  \n",
    "\n",
    "But for comparison purposes, we will finetune it to see how much of an improvement we can get and to confirm whether or not this model can offer competitive results compared with the others.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d19b6f",
   "metadata": {},
   "source": [
    "Given the small dataset instead of using doing cross validation with k-fold, which would reduce even more the training data, we will keep it simpler with a manual grid search, looping through different random seeds to reduce variance on the results.  \n",
    "\n",
    "We will repeat the process (build-train-evaluate) for different sets of values (number of epochs, batch sizes and learning rates) and we will do it with different seeds averaging the performance metrics obtained (mean of RMSE and its standard deviation). This is necessary to improve generalization and ensure we dont get a good result just due to luck with a lucky initial state (random seed).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f376df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [20, 100]\n",
    "batch_sizes = [16, 32, 64]\n",
    "learning_rates = [1e-3, 1e-2]\n",
    "random_seeds = [11, 33, 42, 55, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for early stopping and learning rate scheduler\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=7, restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_results = [] # Create an empty list to store results\n",
    "\n",
    "# Grid search loop\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for ep in epochs_list:\n",
    "            val_rmse_list = [] # List to store RMSE for each random seed\n",
    "            epochs_trained_list = [] # List to store number of epochs trained for each random seed\n",
    "            for seed in random_seeds:\n",
    "                \n",
    "                tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "                # New model instance\n",
    "                MLP_model = MLP_instance(num_features, cat_features, X_train)\n",
    "\n",
    "                # Compile model\n",
    "                MLP_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss='mse',\n",
    "                    metrics=[RootMeanSquaredError()]\n",
    "                )\n",
    "\n",
    "                # Fit the model\n",
    "                trained_model = MLP_model.fit(\n",
    "                    X_train_inputs, Y_train.values,\n",
    "                    validation_data=(X_dev_inputs, Y_dev.values),\n",
    "                    epochs=ep,\n",
    "                    batch_size=bs,\n",
    "                    callbacks=[early_stop, lr_scheduler],\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate\n",
    "                loss, rmse = MLP_model.evaluate(X_dev_inputs, Y_dev.values, verbose=0)\n",
    "\n",
    "                val_rmse_list.append(rmse) # Append RMSE to the list\n",
    "                epochs_trained_list.append(len(trained_model.history['loss'])) # Append number of epochs trained to the list\n",
    "\n",
    "                del MLP_model # Clear the model from memory\n",
    "                tf.keras.backend.clear_session() # Clear the session to free up resources\n",
    "\n",
    "            # Store result in a dict and append to the results list\n",
    "            MLP_results.append({\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": bs,\n",
    "                \"epochs_requested\": ep,\n",
    "                \"epochs_trained_mean\": np.mean(epochs_trained_list),\n",
    "                \"val_rmse_mean\": np.mean(val_rmse_list),\n",
    "                \"val_rmse_std\": np.std(val_rmse_list),\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "MLP_results_df = pd.DataFrame(MLP_results)\n",
    "\n",
    "# Sort by best RMSE\n",
    "MLP_results_df = MLP_results_df.sort_values(by=\"val_rmse_mean\")\n",
    "\n",
    "# Show best settings:\n",
    "print(\"\\nTop 10 settings by RMSE:\")\n",
    "MLP_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ce292",
   "metadata": {},
   "source": [
    "We will go forward with the best option and train again the model for a final evaluation on the holdout set, for later comparison with the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915334f8",
   "metadata": {},
   "source": [
    "Model building and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP_instance(num_features, cat_features, X_train)\n",
    "MLP_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse', metrics=[RootMeanSquaredError()])\n",
    "MLP_model.fit(X_train_inputs, Y_train.values, validation_data=(X_dev_inputs, Y_dev.values), epochs=100, batch_size=16, callbacks=[early_stop, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba13987",
   "metadata": {},
   "source": [
    "Model evaluation on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = MLP_model.predict(X_dev_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_dev_rmse_log = root_mean_squared_error(Y_dev, Y_pred)\n",
    "MLP_dev_rmse_dollars = root_mean_squared_error(np.expm1(Y_dev), np.expm1(Y_pred))\n",
    "print(f\"MLP RMSE (log scale): {MLP_dev_rmse_log:.4f}\")\n",
    "print(f\"MLP RMSE (dollars): {MLP_dev_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4466d64",
   "metadata": {},
   "source": [
    "And finally, we will transform the holdout set just as we did with the train and dev sets, and perform the last evaluation on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc791ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hold_inputs = {\n",
    "    col: X_hold[col].values for col in num_features + cat_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = MLP_model.predict(X_hold_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_hold_rmse_log = root_mean_squared_error(Y_hold, Y_pred)\n",
    "MLP_hold_rmse_dollars = root_mean_squared_error(np.expm1(Y_hold), np.expm1(Y_pred))\n",
    "print(f\"MLP RMSE (log scale): {MLP_hold_rmse_log:.4f}\")\n",
    "print(f\"MLP RMSE (dollars): {MLP_hold_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521fc95",
   "metadata": {},
   "source": [
    "Finally, lets store our performance results and save our MLP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary.append({\n",
    "    'model': 'MLP model',\n",
    "    'dev_rmse_log': MLP_dev_rmse_log,\n",
    "    'dev_rmse_dollars': MLP_dev_rmse_dollars,\n",
    "    'holdout_rmse_log': MLP_hold_rmse_log,\n",
    "    'holdout_rmse_dollars': MLP_hold_rmse_dollars\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c924d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.save(\"../models/mlp_model.keras\")\n",
    "\n",
    "del MLP_model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the model\n",
    "# MLP_model = tf.keras.models.load_model(\"../models/mlp_model.keras\", custom_objects={'RootMeanSquaredError': RootMeanSquaredError})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacd401",
   "metadata": {},
   "source": [
    "### 5.5 Multi Level Dense Layer NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25e6cd",
   "metadata": {},
   "source": [
    "Multi Level Dense Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5680885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLDL_instance(num_features, cat_features, X_train):\n",
    "    # Input layer\n",
    "    # This will create a dictionary which contains an input layer per feature, to help us preprocess them individually\n",
    "    inputs = {}\n",
    "    for name in num_features + cat_features:\n",
    "        inputs[name] = Input(shape=(1,), name=name, dtype='float32' if name in num_features else 'string')\n",
    "\n",
    "    # Preprocessing for numerical\n",
    "    norm_layers = {}\n",
    "    for feature in num_features:\n",
    "        norm = Normalization() # Create a Normalization layer\n",
    "        norm.adapt(X_train[feature].values.reshape(-1, 1))  # Gets the mean and variance of the training data, reshape needed to transform (n_samples,) to (n_samples, 1)\n",
    "        norm_layers[feature] = norm(inputs[feature]) # Apply normalization to the input feature\n",
    "\n",
    "    # Preprocessing for categorical\n",
    "    cat_layers = {}\n",
    "    for feature in cat_features:\n",
    "        lookup = StringLookup(output_mode='int') # Layer to map strings to integers\n",
    "        lookup.adapt(X_train[feature].values) # Learns all unique values (categories) of the feature to map them to integers\n",
    "        \n",
    "        vocab_size = lookup.vocabulary_size() # Number of unique values (categories) in the feature + 1 for the UNK token\n",
    "        encoding = CategoryEncoding(output_mode='one_hot', num_tokens=vocab_size) # Layer to convert the integer encoded feature to one-hot encoding\n",
    "        \n",
    "        int_encoded = lookup(inputs[feature]) # Apply the lookup layer to the features\n",
    "        one_hot_encoded = encoding(int_encoded) # Apply the encoding layer to the integer encoded feature\n",
    "        cat_layers[feature] = one_hot_encoded # Store the one-hot encoded tensor to the dict\n",
    "\n",
    "    # Combine all features\n",
    "    all_features = list(norm_layers.values()) + list(cat_layers.values())\n",
    "    x = Concatenate()(all_features)\n",
    "\n",
    "    # Split into 3 branches for multi-level dense layers\n",
    "    branch_1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    branch_1 = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(branch_1)\n",
    "    branch_1 = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(branch_1)\n",
    "\n",
    "    branch_2 = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "\n",
    "    branch_3 = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "\n",
    "    # Merge branches\n",
    "    merged = Concatenate()([branch_1, branch_2, branch_3])\n",
    "    merged = Dense(8, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merged)\n",
    "    output = Dense(1)(merged)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6af3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for early stopping and learning rate scheduler\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=7, restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf966776",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDL_model = MLDL_instance(num_features, cat_features, X_train)\n",
    "MLDL_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                     loss='mse',\n",
    "                     metrics=[RootMeanSquaredError()])\n",
    "\n",
    "MLDL_model.fit(X_train_inputs, Y_train.values,\n",
    "                 validation_data=(X_dev_inputs, Y_dev.values),\n",
    "                 epochs=100,\n",
    "                 batch_size=16,\n",
    "                 callbacks=[early_stop, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b135e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, rmse = MLDL_model.evaluate(X_dev_inputs, Y_dev.values)\n",
    "print(f\"Multi Level Dense Layer NN RMSE (log scale): {rmse:.4f}, loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [20, 100]\n",
    "batch_sizes = [16, 32, 64]\n",
    "learning_rates = [1e-3, 1e-2]\n",
    "random_seeds = [11, 33, 42, 55, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDL_results = [] # Create an empty list to store results\n",
    "\n",
    "# Grid search loop\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for ep in epochs_list:\n",
    "            val_rmse_list = [] # List to store RMSE for each random seed\n",
    "            epochs_trained_list = [] # List to store number of epochs trained for each random seed\n",
    "            for seed in random_seeds:\n",
    "                \n",
    "                tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "                # New model instance\n",
    "                MLDL_model = MLDL_instance(num_features, cat_features, X_train)\n",
    "\n",
    "                # Compile model\n",
    "                MLDL_model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss='mse',\n",
    "                    metrics=[RootMeanSquaredError()]\n",
    "                )\n",
    "\n",
    "                # Fit the model\n",
    "                trained_model = MLDL_model.fit(\n",
    "                    X_train_inputs, Y_train.values,\n",
    "                    validation_data=(X_dev_inputs, Y_dev.values),\n",
    "                    epochs=ep,\n",
    "                    batch_size=bs,\n",
    "                    callbacks=[early_stop, lr_scheduler],\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate\n",
    "                loss, rmse = MLDL_model.evaluate(X_dev_inputs, Y_dev.values, verbose=0)\n",
    "\n",
    "                val_rmse_list.append(rmse) # Append RMSE to the list\n",
    "                epochs_trained_list.append(len(trained_model.history['loss'])) # Append number of epochs trained to the list\n",
    "\n",
    "                del MLDL_model # Clear the model from memory\n",
    "                tf.keras.backend.clear_session() # Clear the session to free up resources\n",
    "\n",
    "            # Store result in a dict and append to the results list\n",
    "            MLDL_results.append({\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": bs,\n",
    "                \"epochs_requested\": ep,\n",
    "                \"epochs_trained_mean\": np.mean(epochs_trained_list),\n",
    "                \"val_rmse_mean\": np.mean(val_rmse_list),\n",
    "                \"val_rmse_std\": np.std(val_rmse_list),\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "MLDL_results_df = pd.DataFrame(MLDL_results)\n",
    "\n",
    "# Sort by best RMSE\n",
    "MLDL_results_df = MLDL_results_df.sort_values(by=\"val_rmse_mean\")\n",
    "\n",
    "# Show best settings:\n",
    "print(\"\\nTop 10 settings by RMSE:\")\n",
    "MLDL_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "del MLDL_model\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDL_model = MLDL_instance(num_features, cat_features, X_train)\n",
    "MLDL_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse', metrics=[RootMeanSquaredError()])\n",
    "MLDL_model.fit(X_train_inputs, Y_train.values, validation_data=(X_dev_inputs, Y_dev.values), epochs=150, batch_size=16, callbacks=[early_stop, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = MLDL_model.predict(X_dev_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDL_dev_rmse_log = root_mean_squared_error(Y_dev, Y_pred)\n",
    "MLDL_dev_rmse_dollars = root_mean_squared_error(np.expm1(Y_dev), np.expm1(Y_pred))\n",
    "print(f\"MLDL RMSE (log scale): {MLDL_dev_rmse_log:.4f}\")\n",
    "print(f\"MLDL RMSE (dollars): {MLDL_dev_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = MLDL_model.predict(X_hold_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDL_hold_rmse_log = root_mean_squared_error(Y_hold, Y_pred)\n",
    "MLDL_hold_rmse_dollars = root_mean_squared_error(np.expm1(Y_hold), np.expm1(Y_pred))\n",
    "print(f\"MLDL RMSE (log scale): {MLDL_hold_rmse_log:.4f}\")\n",
    "print(f\"MLDL RMSE (dollars): {MLDL_hold_rmse_dollars:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ef942",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary.append({\n",
    "    'model': 'MLDL model',\n",
    "    'dev_rmse_log': MLDL_dev_rmse_log,\n",
    "    'dev_rmse_dollars': MLDL_dev_rmse_dollars,\n",
    "    'holdout_rmse_log': MLDL_hold_rmse_log,\n",
    "    'holdout_rmse_dollars': MLDL_hold_rmse_dollars\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLDL_model.save(\"../models/mldl_model.keras\")\n",
    "\n",
    "del MLDL_model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the model\n",
    "# MLDL_model = tf.keras.models.load_model(\"../models/mldl_model.keras\", custom_objects={'RootMeanSquaredError': RootMeanSquaredError})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0aff91",
   "metadata": {},
   "source": [
    "## 6. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331198f",
   "metadata": {},
   "source": [
    "We have been storing the metrics of the tuned models, lets order them by performance and see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b719c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary_df = pd.DataFrame(model_summary)\n",
    "model_summary_df = model_summary_df.sort_values(by=\"holdout_rmse_log\") # Sort by best RMSE (log scale) on the holdout set\n",
    "\n",
    "model_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6caba",
   "metadata": {},
   "source": [
    "As we can see our CatBoost Regressor model outperforms all others, which is to be expected given the size of the dataset and the nature of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c512d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model summary to a CSV file\n",
    "model_summary_df.to_csv('../outputs/comparison_summary.csv', index=False)\n",
    "\n",
    "# Load the model summary\n",
    "# model_summary_df = pd.read_csv('../outputs/comparison_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958acbba",
   "metadata": {},
   "source": [
    "## 7. Final model prediction & submission\n",
    "### 7.1 Retrain on full training set\n",
    "### 7.2 Predict on test set\n",
    "### 7.3 Reverse log-transformation\n",
    "### 7.4 Generate submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
